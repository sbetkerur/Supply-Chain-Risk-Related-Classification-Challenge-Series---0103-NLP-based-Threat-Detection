{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###                                          **Supply Chain - NLP based Threat Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses the implementation of **BERT from the TensorFlow Models** repository on GitHub at tensorflow/models/official/nlp/bert. It uses L=12 hidden layers (Transformer blocks), a hidden size of H=768, and A=12 attention heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration Section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_training_master_filepath = 'master_train.csv'\n",
    "input_training_node_filepath = 'node_train.csv'\n",
    "input_test_master_filepath = 'master_test.csv'\n",
    "input_test_node_filepath = 'node_test.csv'\n",
    "\n",
    "Output_file_path = 'result_submission.csv'\n",
    "\n",
    "bert_module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2' # default path '\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "import tensorflow_hub as hub\n",
    "import tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Function 1 for `BERT Layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode text into tokens, masks, and segment flags\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=128):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        \n",
    "        text = tokenizer.tokenize(text)\n",
    "        \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Function 2 for `BERT Layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model(bert_layer, max_len=128):\n",
    "    \n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ClassificationReport` which is similar to `sklearn.metrics.classification_report`, computes **Precision, Recall and F1-Score** metrics after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClassificationReport(Callback):\n",
    "    \n",
    "    def __init__(self, train_data=(), validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        \n",
    "        self.X_train, self.y_train = train_data\n",
    "        self.train_precision_scores = []\n",
    "        self.train_recall_scores = []\n",
    "        self.train_f1_scores = []\n",
    "        \n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.val_precision_scores = []\n",
    "        self.val_recall_scores = []\n",
    "        self.val_f1_scores = [] \n",
    "               \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        train_predictions = np.round(self.model.predict(self.X_train, verbose=0))\n",
    "        \n",
    "        train_predictions.astype(int)\n",
    "        \n",
    "        np.reshape(self.y_train,(self.y_train.size))\n",
    "        np.reshape(train_predictions,(train_predictions.size))\n",
    "        \n",
    "        train_precision = precision_score(self.y_train, train_predictions, average='macro',zero_division=1)\n",
    "        train_recall = recall_score(self.y_train, train_predictions, average='macro',zero_division=1)\n",
    "        train_f1 = f1_score(self.y_train, train_predictions, average='macro',zero_division=1)\n",
    "        self.train_precision_scores.append(train_precision)        \n",
    "        self.train_recall_scores.append(train_recall)\n",
    "        self.train_f1_scores.append(train_f1)\n",
    "        \n",
    "        val_predictions = np.round(self.model.predict(self.X_val, verbose=0))\n",
    "        val_precision = precision_score(self.y_val, val_predictions, average='macro',zero_division=1)\n",
    "        val_recall = recall_score(self.y_val, val_predictions, average='macro',zero_division=1)\n",
    "        val_f1 = f1_score(self.y_val, val_predictions, average='macro',zero_division=1)\n",
    "        self.val_precision_scores.append(val_precision)        \n",
    "        self.val_recall_scores.append(val_recall)        \n",
    "        self.val_f1_scores.append(val_f1)\n",
    "        \n",
    "        print('\\nEpoch: {} - Training Precision: {:.6} - Training Recall: {:.6} - Training F1: {:.6}'.format(epoch + 1, train_precision, train_recall, train_f1))\n",
    "        print('Epoch: {} - Validation Precision: {:.6} - Validation Recall: {:.6} - Validation F1: {:.6}'.format(epoch + 1, val_precision, val_recall, val_f1))  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning and formatting for `Training Data`.Create `Train` `Validation` set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_cleaning():\n",
    "    \n",
    "    train_csv = pd.read_csv(input_training_master_filepath)\n",
    "    train_nodes_csv = pd.read_csv(input_training_node_filepath)\n",
    "    \n",
    "    count_title_subject_difference = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # 'Id' field is the link between the 2 csv files. truncate left and right spaces\n",
    "    train_nodes_csv['Id'] = train_nodes_csv['Id'].str.strip()\n",
    "    train_csv['Id'] = train_csv['Id'].str.strip()\n",
    "    \n",
    "    # create fields in the primary csv df for the data to be populated from node csv df\n",
    "    train_csv['City'] = ''\n",
    "    train_csv['Country'] = ''\n",
    "    train_csv['Node Type'] = ''\n",
    "    train_csv['Status'] = ''\n",
    "    \n",
    "    # populate City, Node Type, Country from the node csv df to the primary df\n",
    "    for i,row in train_csv.iterrows():\n",
    "        id_node = train_nodes_csv.loc[train_nodes_csv['Id'] == row['Id']]\n",
    "        for i1,row1 in id_node.iterrows():\n",
    "            if str(row1['City']) not in str(row['City']):\n",
    "                train_csv.loc[i,'City'] = str(train_csv.loc[i,'City']) + ' ' + str(row1['City'])\n",
    "            if str(row1['Node Type']) not in str(train_csv.loc[i,'Node Type']):\n",
    "                train_csv.loc[i,'Node Type'] = str(train_csv.loc[i,'Node Type']) + ' ' + str(row1['Node Type'])\n",
    "            if (len(str(row1['Country'])) > 0):\n",
    "                train_csv.loc[i,'Country'] = str(row1['Country'])\n",
    "                country_str = str(row1['Country']).strip()\n",
    "            if (len(str(row1['Status'])) > 0):\n",
    "                train_csv.loc[i,'Status'] = str(row1['Status'])\n",
    "\n",
    "    # check if 'Title' and 'Subject' fields contains same data. If different concatenate\n",
    "        title_str = row['Title'].replace(' ','')\n",
    "        subject_str = row['Subject'].replace(' ','')\n",
    "        if title_str != subject_str:\n",
    "            count_title_subject_difference += 1\n",
    "            row['Title'] = row['Title'] + '. ' + row['Subject']\n",
    "        total_count += 1\n",
    "\n",
    "        # clean 'Title' field\n",
    "        row['Title'] = row['Title'].replace(country_str,'')\n",
    "        row['Title'] = row['Title'].replace('Incident ','')\n",
    "        row['Title'] = row['Title'].replace('Moderate ','')\n",
    "        row['Title'] = row['Title'].replace('Severe ','')\n",
    "        row['Title'] = row['Title'].replace('Minor ','')\n",
    "        row['Title'] = row['Title'].replace('Extreme ','')\n",
    "        train_csv.loc[i,'Title'] = row['Title']\n",
    "\n",
    "    print(\"Total No. of rows in Training Data: \", total_count)\n",
    "    print(\"No. of rows with Title & Subject different in train Data: \", count_title_subject_difference)\n",
    "    \n",
    "    train = pd.DataFrame()\n",
    "    \n",
    "    # concatenate fields for creating text to be processed by BERT model\n",
    "    train_csv['text'] = train_csv['Severity'].map(str) + '. ' + train_csv['Status'].map(str) + '. ' + train_csv['Country'].map(str) + '.' +  train_csv['City'].map(str) + '. ' + train_csv['Category'].map(str) + '.' +  train_csv['Node Type'].map(str) + '.' + train_csv['Title'].map(str) + '. ' + train_csv['Summary'].map(str)\n",
    "    \n",
    "    # create train, validation and test df\n",
    "    train = train_csv[['Alert ID','text']]\n",
    "    Xtrain_input = train['text']\n",
    "    Ytrain_labels = train['Alert ID']\n",
    "\n",
    "    train_input, val_input, train_labels, val_labels = train_test_split(Xtrain_input, Ytrain_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    return train_input, val_input, train_labels, val_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning and formatting for `Test Data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_cleaning():\n",
    "    \n",
    "    test_input = pd.DataFrame()\n",
    "    test_csv = pd.read_csv(input_test_master_filepath)\n",
    "    test_nodes_csv = pd.read_csv(input_test_node_filepath)\n",
    "    \n",
    "    count_title_subject_difference = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    # 'Id' field is the link between the 2 csv files. truncate left and right spaces\n",
    "    test_nodes_csv['Id'] = test_nodes_csv['Id'].str.strip()\n",
    "    test_csv['Id'] = test_csv['Id'].str.strip()\n",
    "    \n",
    "    # create fields in the primary csv df for the data to be populated from node csv df\n",
    "    test_csv['City'] = ''\n",
    "    test_csv['Country'] = ''\n",
    "    test_csv['Node Type'] = ''\n",
    "    test_csv['Status'] = ''\n",
    "    \n",
    "    # populate City, Node Type, Country from the node csv df to the primary df\n",
    "    for i,row in test_csv.iterrows():\n",
    "        id_node = test_nodes_csv.loc[test_nodes_csv['Id'] == row['Id']]\n",
    "        for i1,row1 in id_node.iterrows():\n",
    "            if str(row1['City']) not in str(row['City']):\n",
    "                test_csv.loc[i,'City'] = str(test_csv.loc[i,'City']) + ' ' + str(row1['City'])\n",
    "            if str(row1['Node Type']) not in str(test_csv.loc[i,'Node Type']):\n",
    "                test_csv.loc[i,'Node Type'] = str(test_csv.loc[i,'Node Type']) + ' ' + str(row1['Node Type'])\n",
    "            if (len(str(row1['Country'])) > 0):\n",
    "                test_csv.loc[i,'Country'] = str(row1['Country'])\n",
    "                country_str = str(row1['Country']).strip()\n",
    "            if (len(str(row1['Status'])) > 0):\n",
    "                test_csv.loc[i,'Status'] = str(row1['Status'])\n",
    "\n",
    "        # check if 'Title' and 'Subject' fields contains same data. If different concatenate\n",
    "        title_str = row['Title'].replace(' ','')\n",
    "        subject_str = row['Subject'].replace(' ','')\n",
    "        if title_str != subject_str:\n",
    "            count_title_subject_difference += 1\n",
    "            row['Title'] = row['Title'] + '. ' + row['Subject']\n",
    "        total_count += 1\n",
    "\n",
    "        # clean 'Title' field\n",
    "        row['Title'] = row['Title'].replace(country_str,'')\n",
    "        row['Title'] = row['Title'].replace('Incident ','')\n",
    "        row['Title'] = row['Title'].replace('Moderate ','')\n",
    "        row['Title'] = row['Title'].replace('Severe ','')\n",
    "        row['Title'] = row['Title'].replace('Minor ','')\n",
    "        row['Title'] = row['Title'].replace('Extreme ','')\n",
    "        test_csv.loc[i,'Title'] = row['Title']\n",
    "\n",
    "    print(\"Total No. of rows in Test Data: \", total_count)\n",
    "    print(\"No. of rows with Title & Subject different in Test Data: \", count_title_subject_difference)\n",
    "    \n",
    "    \n",
    "    # concatenate fields for creating text to be processed by BERT model\n",
    "    test_csv['text'] =  test_csv['Severity'].map(str) + '. ' + test_csv['Status'].map(str) + '. ' + test_csv['Country'].map(str) + '.' +  test_csv['City'].map(str) + '. ' + test_csv['Category'].map(str) + '.' +  test_csv['Node Type'].map(str) + '.' + test_csv['Title'].map(str) + '. ' + test_csv['Summary'].map(str)\n",
    "    test_input['text'] = test_csv['text']\n",
    "    return test_input, test_csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `BERT` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert_layer():\n",
    "    bert_layer = hub.KerasLayer(bert_module_url, trainable=True)\n",
    "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "    return tokenizer, bert_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the text into tokens, masks, and segment flags (`BERT`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def encode_tokens(tokenizer, train_input, val_input, test_input):\n",
    "    train_input = bert_encode(train_input.values, tokenizer, max_len=128)\n",
    "    val_input = bert_encode(val_input.values, tokenizer, max_len=128)\n",
    "    test_input = bert_encode(test_input.text.values, tokenizer, max_len=128)\n",
    "    return train_input, val_input, test_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Build, train and validate model` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model_default_option(bert_layer, train_input, val_input, train_labels, val_labels):\n",
    "    \n",
    "    model = build_model(bert_layer, max_len=128)\n",
    "    model.summary()\n",
    "    \n",
    "    metrics = ClassificationReport(train_data=(train_input, train_labels), validation_data=(val_input, val_labels))\n",
    "\n",
    "    train_history = model.fit(\n",
    "        train_input, train_labels,\n",
    "        validation_data=(val_input, val_labels),\n",
    "        epochs=3,\n",
    "        batch_size=32,\n",
    "        callbacks=[metrics]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Model` Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_model(model, test_input, test_csv):\n",
    "    \n",
    "    test_pred = model.predict(test_input)\n",
    "    \n",
    "    test_csv.drop(columns=['City','Country','Node Type','text','Status'], inplace=True,axis=1)\n",
    "    test_csv['Threat Level'] = test_pred\n",
    "    test_csv['Alert ID'] = np.round(test_pred)\n",
    "    \n",
    "    test_csv.to_csv(Output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Main Processing Section`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data being processed\n",
      "Total No. of rows in Training Data:  23518\n",
      "No. of rows with Title & Subject different in train Data:  0\n",
      "Train data processing completed\n",
      "Total No. of rows in Test Data:  11861\n",
      "No. of rows with Title & Subject different in Test Data:  0\n",
      "Test data processing completed\n",
      "BERT layer loaded\n",
      "BERT encode tokens completed\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 768)]        0           keras_layer_1[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            769         tf_op_layer_strided_slice_1[0][0]\n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 109,483,009\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "Train on 18814 samples, validate on 4704 samples\n",
      "Epoch 1/3\n",
      "18784/18814 [============================>.] - ETA: 22s - loss: 0.0484 - accuracy: 0.9830\n",
      "Epoch: 1 - Training Precision: 0.932493 - Training Recall: 0.658596 - Training F1: 0.73123\n",
      "Epoch: 1 - Validation Precision: 0.905575 - Validation Recall: 0.606928 - Validation F1: 0.667941\n",
      "18814/18814 [==============================] - 20955s 1s/sample - loss: 0.0483 - accuracy: 0.9830 - val_loss: 0.0232 - val_accuracy: 0.9926\n",
      "Epoch 2/3\n",
      "18784/18814 [============================>.] - ETA: 21s - loss: 0.0139 - accuracy: 0.9962\n",
      "Epoch: 2 - Training Precision: 0.998824 - Training Recall: 0.851351 - Training F1: 0.91211\n",
      "Epoch: 2 - Validation Precision: 0.997864 - Validation Recall: 0.761905 - Validation F1: 0.84268\n",
      "18814/18814 [==============================] - 20524s 1s/sample - loss: 0.0139 - accuracy: 0.9962 - val_loss: 0.0155 - val_accuracy: 0.9957\n",
      "Epoch 3/3\n",
      "18784/18814 [============================>.] - ETA: 21s - loss: 0.0060 - accuracy: 0.9980\n",
      "Epoch: 3 - Training Precision: 0.996394 - Training Recall: 0.983081 - Training F1: 0.989646\n",
      "Epoch: 3 - Validation Precision: 0.971365 - Validation Recall: 0.904547 - Validation F1: 0.935362\n",
      "18814/18814 [==============================] - 20409s 1s/sample - loss: 0.0060 - accuracy: 0.9980 - val_loss: 0.0107 - val_accuracy: 0.9979\n",
      "Model training completed\n",
      "The model is trained and has predicted the values. File Path:  result_submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Train data being processed\")\n",
    "train_input, val_input, train_labels, val_labels = train_data_cleaning()\n",
    "print(\"Train data processing completed\")\n",
    "test_input, test_csv = test_data_cleaning()\n",
    "print(\"Test data processing completed\")\n",
    "tokenizer, bert_layer =load_bert_layer()\n",
    "print(\"BERT layer loaded\")\n",
    "train_input, val_input, test_input = encode_tokens(tokenizer, train_input, val_input, test_input)\n",
    "print(\"BERT encode tokens completed\")\n",
    "model =  train_model_default_option(bert_layer, train_input, val_input, train_labels, val_labels)\n",
    "print(\"Model training completed\")\n",
    "predict_model(model, test_input, test_csv)\n",
    "print(\"The model is trained and has predicted the values. File Path: \",Output_file_path)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
